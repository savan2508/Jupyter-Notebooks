{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression \n",
    "* Linear regression is not a new concept and has been around the talk of mathematicians for ages. Here are a few historical events regarding linear regression: \n",
    "    * 1722 - Roger Cotes discovers combining different observations yields better estimates of the true value. \n",
    "    * 1750 - Tobias Mayer explores averaging different results under similar conditions in studying liberations of the moon. \n",
    "    * 1757 - Roger Joseph Boscovich further develops combining observations studying the shape of the Earth. \n",
    "    * 1788 - Pierre-Simon LaPlace develops similar averaging theories in explaining the difference between Jupiter and Saturn. \n",
    "    * 1805 - First public exposition on Linear Regression with least squares methods published by Adrien-Marie Legendre-Nouvelles Metheodes pour La Detemination des Orbites des Cometes. \n",
    "    * 1809 - Carl Friedrich Gauss publishes his methods of calculating orbits of celestial bodies, and claimed to have invented least-squares back in 1795. \n",
    "* A linear relationship implies some constant straight-line relationship. The simplest possible is y = x. \n",
    "* It would have been great if in real life we can get data that can fit within the same line or have one-to-one relationship. But often time we get data that are scattered around the plot and we have to find the best solution to find the best relation between x and y. To draw a perfect line that can fit all the data, we should try to minimize the distance from each data point to the line. In simple terms, the distance from the line to the data points is the residual error, and we want to minimize these errors. \n",
    "\n",
    "## Ordinary Least Squares: \n",
    "* One method to minimize the errors is to use ordinary least squares. Ordinary least squares work by minimizing the sum of the squares of the differences between the observed dependent variable (value of the variable being observed) in the given dataset and those predicted by the linear function. \n",
    "* Having a squared error will help us simplify calculations when setting up derivatives. \n",
    "* The simple equation for the linear straight line is y = mx + c, where y is the dependent variable, x is the independent variable, m is the slope of the line and c is the constant or distance from the origin to the intersection to the y-axis. \n",
    "    * The equation y = mx + c allows us to use only one dependent and one independent variable to analyze our data and make our predictions. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
