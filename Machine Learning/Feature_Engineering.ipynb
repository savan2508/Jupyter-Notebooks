{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering:\n",
    "* Feature engineering is the process of using domain knowledge to extract feature from raw data via data mining techniques. \n",
    "* There are three approaches we can take:\n",
    "    * Extracting information\n",
    "    * Combining information\n",
    "    * Transforming information\n",
    "### Extracting information:\n",
    "* Imagine we have a some dataset let's say from the shop and its expenditure. In that dataset we have a timestamp for each row in the following format: 2023-01-07 08:12:23. The timestamp in the current format is very difficult to pass into a machine learning algorithm. \n",
    "* There is no coefficient we can apply for a non-numeric data point. In general for most algorithms we need to make sure features are float or int. \n",
    "    * In this data we may do something like this:\n",
    "        * For the year we can create a year variable (year: 2023)\n",
    "        * Month: 12\n",
    "        * Weekday or Weekend (0/1 or true/false)\n",
    "        * Mon:1, Tue:1, ...\n",
    "* Another example may be in complex dataset we have a text data for deed of house, in that case we can do something like measure a length of the text or may be have our algorithm to train to look for certain key word and keep track of it that how many time that word is repeated. \n",
    "\n",
    "### Combining information: \n",
    "* We have done something similar in the linear regression, where we have combined the data of radio, tv and newspaper spending into the total spending feature to create our model, and also we have done that in the extraction feature we have multiplied each feature with each other to see the codependency. \n",
    "\n",
    "### Transforming information: \n",
    "* Transforming information is commonly used for the string data. As we have discussed above that when we have data set such as house deed, we can't directly feed that information to the machine since algorithms do not know what to do with it in its raw form. So we need to transform that data to use it in our machine learning model. \n",
    "* Often categorical data is presented as string data. For example a large data set of social network users could have country of origin as a string feature (such as USA, UK, IND, etc...). \n",
    "* We can use two approaches here: \n",
    "    * Integer Encoding: \n",
    "        * Directly converts categories into integers such as USA: 1, UK: 2, IND: 3, etc...\n",
    "        * We need to understand when doing something like integer encoding because when we feed the data to machine we are creating something called ordinal variable. In this case machine may think that the data we have provided is that UK is worth twice of USA and IND worth three times of USA. \n",
    "        * In certain cases we want that outcome where assigning values to string makes more sense. For example, for spice level dataset we can assign following values mild: 1, hot: 2, fire: 3.  \n",
    "        * There are certain pros of this method such as this is very easy to understand, and does not increase number of features in our model. \n",
    "        * But as we saw there is one major downside of implying ordered relationship between categories. \n",
    "    * One-hot Encoding (Dummy variables):\n",
    "        *  The dummy variable method is that we can introduce a new feature for each categories and assign it a value 0 or 1 based on the if that raw contains that feature of not. But here we can see how much complex our model can be if we use this method.  \n",
    "        * Using pandas .map() or .apply() we can easily achieve this. May require a tuning and domain experience to choose reasonable higher level categories or mappings. \n",
    "        * We must be aware of the dummy variable trap, mathematically known as multi-collinearity. Converting to dummy variables can cause features to be duplicated. \n",
    "\n",
    "## Dealing with outliers:\n",
    "* Often a data set will have a few points that are extreme outliers. There is no right answer to how to deal with them, but in most cases it is advisable to drop these values in order to have a more generalized model. \n",
    "* Outlier consideration: \n",
    "    * Definition of an outlier: The definition of the outlier is strongly dependent on the dataset, and there is not fix answer that fits to all. It is advisable to consult with the expert if not have strong familiarization with the dataset. \n",
    "        * Range and Limits: \n",
    "            * We need to decide what will constitute an outlier with some methodology: \n",
    "                * InterQuartile Range\n",
    "                * Standard Deviation\n",
    "                * Visualized or Domain Limit value\n",
    "        * Percentage of data\n",
    "            * We need to keep that in mind when defining the percentage of data as an outlier, because that actually just have a wide distribution, not outliers. \n",
    "            * Limit outliers to a few percentage points a most. \n",
    "* When dealing with outliers we should utilize visualization plots to be able to see and identify outlie points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
